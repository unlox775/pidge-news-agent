Context.add_conversation(:bird)
Context.add_conversation(:parser)
Context.add_conversation(:reader)

search_intent = ai_prompt(:bird, "bird/00_confirm_search_intent", human_input: true)

new_search_sites =
  search_intent
  |> ai_object_extract(:bird, "bird/01_get_new_sites_and_terms", human_input: true)
search_sites.search_engines <~ new_search_sites.search_engines

# Compile a parser_js for each site
foreach(search_sites.search_engines, fn {site, i} ->
  if site.parser_js do
    continue
  end

  dom = nil
  case site.type do
    "no_search" -> dom = load_url_dom(site.url)
    "search_url" ->
      url = Local.swap_url_term(site.url, site.search_term[0])
      dom = load_url_dom(url)
    "search_page" ->
      dom = try_all_inputs_enterkey(site.search_term[0])
  end

  first_article_strings =
    dom
    |> extract_strings()
    |> ai_object_extract(:bird, "bird/02_find_first_article_text")

  first_article_dom =
    dom
    |> find_subdom(string: first_article_strings.article_name, ext_strings: first_article_strings.first_few_words_of_each_piece_of_text_in_article_listing)

  parser_js = ai_prompt(:parser, "parser/03_generate_detect_js") |> js_script_extract()
  all_articles =
    dom
    |> dom_js_exec(js: parser_js)

  possible_ads =
    |> ai_object_extract(:parser, "parser/04_find_the_ads")

  if count(possible_ads) > 0 then
    first_possible_ad_dom =
      dom
      |> find_subdom(
        string: possible_ads.possible_ads_by_name[0].name,
        ext_strings: [possible_ads.possible_ads_by_name[0].summary]
        )

    parser_js =
      ai_prompt(:parser, "parser/05_revise_the_parser_js")
      |> js_script_extract()

  search_sites[i].parser_js = parser_js
end, async: true)

# Get the most recent articles from each site
foreach(search_sites.search_engines, fn {site, i} ->
  search_urls = []
  case site.type do
    "no_search" -> search_urls = [site.url]
    "search_url" ->
      foreach(site.search_term, fn term ->
        search_urls += Local.swap_url_term(site.url, term)
      end)
    "search_page" ->
      foreach(site.search_term, fn term ->
        search_urls += try_all_inputs_enterkey(site.search_term[0])
      end)
  end

  foreach(search_urls, fn {url, url_i} ->
    dom = load_url_dom(url)

    all_articles =
      dom
      |> dom_js_exec(js: site.parser_js)

    foreach(all_articles, fn {article, article_i} ->
      article_summary =
        load_url_dom(article.url)
        |> extract_strings(limit_words: 50000)
        |> ai_object_extract(:reader, "reader/06_read_article_extract_summary")

      article_summary.url = article.url

      all_articles[article_i].ai_summary = article_summary
    end, async: true)

    db = Local.store_articles_in_db(all_articles, db)
  end)
end, async: false) # false until we have DB concurrency

fly(:report_news)
