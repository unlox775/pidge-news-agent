Context.add_conversation(:bird)
Context.add_conversation(:parser)
Context.add_conversation(:reader)

search_intent = ai_prompt(:bird, "bird/00_confirm_search_intent", human_input: true)

new_search_sites =
  search_intent
  |> ai_object_extract(:bird, "bird/01_get_new_sites_and_terms", :json, optional_human_input: true, schema: Plot)
search_sites.search_engines <~ new_search_sites.search_engines
sites = Local.get_sites_list(search_sites.search_engines)

# Compile a extractor_json for each site
foreach(sites, fn {site, i} ->
  if site.extractor_json do
    continue
  end

  html = nil
  case site.type do
    "no_search" -> html = Local.load_url_html(site.url)
    "search_url" ->
      url = Local.swap_url_term(site.url, site.search_term[0])
      html = Local.load_url_html(url)
    "search_page" ->
      html = Local.try_all_inputs_enterkey(site.search_term[0])
  end

  page_as_markdown = Local.to_markdown(html, 2000, 18000)

  first_articles_strings =
    page_as_markdown
    |> ai_object_extract(:bird, "bird/02_find_first_articles_text", :json, schema: Plot)

  first_article_html = Local.find_subhtml(
    html,
    first_articles_strings.first.article_name,
    first_articles_strings.first.first_few_words_of_each_piece_of_text_in_article_listing
    )

  extractor_json =
    ai_codeblock_extract(:parser, "parser/03.1_generate_detect_js", :javascript, largest: true)
    |> ai_object_extract(:parser, "parser/03.2_js_to_json_extractor", :json, schema: ObjectExtractor)
  all_articles = Local.dom_extractor(html, extractor_json)

  possible_ads =
    ai_object_extract(:parser, "parser/04_find_the_ads", :json, schema: Plot)

  if possible_ads.length > 0 do
    first_possible_ad_html =
      html
      |> Local.find_subhtml(
        possible_ads.possible_ads_by_name[0].name,
        possible_ads.possible_ads_by_name[0].summary
        )

    extractor_json =
      ai_codeblock_extract(:parser, "parser/05_revise_the_extractor_json", :javascript, largest: true)
  end

  search_sites[i].extractor_json = extractor_json
end) # , async: true

# Get the most recent articles from each site
foreach(search_sites.search_engines, fn {site, i} ->
  search_urls = []
  case site.type do
    "no_search" -> search_urls = [site.url]
    "search_url" ->
      foreach(site.search_term, fn {term,i} ->
        search_urls <~ Local.swap_url_term(site.url, term)
      end)
    "search_page" ->
      foreach(site.search_term, fn {term,i} ->
        search_urls <~ Local.try_all_inputs_enterkey(site.search_term[0])
      end)
  end

  foreach(search_urls, fn {url, url_i} ->
    html = Local.load_url_html(url)

    all_articles = Local.dom_extractor(html, extractor_json)

    foreach(all_articles, fn {article, article_i} ->
      html = Local.load_url_html(article.url)
      article_summary =
        Local.to_markdown(html, 2000, 18000)
        |> ai_object_extract(:reader, "reader/06_read_article_extract_summary", :json, schema: Plot)

      article_summary.url = article.url

      all_articles[article_i].ai_summary = article_summary
    end) # , async: true

    db = Local.store_articles_in_db(all_articles, db)
  end)
end) # , async: false # false until we have DB concurrency

fly(:report_news)
