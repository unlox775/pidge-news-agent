Context.add_conversation(:bird)
Context.add_conversation(:parser)
Context.add_conversation(:reader)

search_intent = ai_prompt(:bird, "bird/00_confirm_search_intent", human_input: true)

new_search_sites =
  search_intent
  |> ai_object_extract(:bird, "bird/01_get_new_sites_and_terms")
search_sites.search_engines <~ new_search_sites.search_engines

# Compile a parser_js for each site
foreach(search_sites.search_engines, fn {site, i} ->
  if site.parser_js do
    continue
  end

  html = nil
  case site.type do
    "no_search" -> html = Local.load_url_html(site.url)
    "search_url" ->
      url = Local.swap_url_term(site.url, site.search_term[0])
      html = Local.load_url_html(url)
    "search_page" ->
      html = Local.try_all_inputs_enterkey(site.search_term[0])
  end

  page_as_markdown = Local.to_markdown(html, 2000)

  first_articles_strings =
    html
    |> Local.to_markdown(2000)
    |> ai_object_extract(:bird, "bird/02_find_first_articles_text")

  first_article_html =
    html
    |> Local.find_subhtml(string: first_article_strings.article_name, ext_strings: first_article_strings.first_few_words_of_each_piece_of_text_in_article_listing)

  parser_js = ai_prompt(:parser, "parser/03_generate_detect_js") |> Local.js_script_extract()
  all_articles =
    html
    |> Local.html_js_exec(js: parser_js)

  possible_ads =
    ai_object_extract(:parser, "parser/04_find_the_ads")

  if count(possible_ads) > 0 do
    first_possible_ad_html =
      html
      |> Local.find_subhtml(
        string: possible_ads.possible_ads_by_name[0].name,
        ext_strings: [possible_ads.possible_ads_by_name[0].summary]
        )

    parser_js =
      ai_prompt(:parser, "parser/05_revise_the_parser_js")
      |> Local.js_script_extract()
  end

  search_sites[i].parser_js = parser_js
end) # , async: true

# Get the most recent articles from each site
foreach(search_sites.search_engines, fn {site, i} ->
  search_urls = []
  case site.type do
    "no_search" -> search_urls = [site.url]
    "search_url" ->
      foreach(site.search_term, fn {term,i} ->
        search_urls <~ Local.swap_url_term(site.url, term)
      end)
    "search_page" ->
      foreach(site.search_term, fn {term,i} ->
        search_urls <~ Local.try_all_inputs_enterkey(site.search_term[0])
      end)
  end

  foreach(search_urls, fn {url, url_i} ->
    html = Local.load_url_html(url)

    all_articles =
      html
      |> Local.html_js_exec(js: site.parser_js)

    foreach(all_articles, fn {article, article_i} ->
      article_summary =
        Local.load_url_html(article.url)
        |> Local.extract_strings(2000)
        |> ai_object_extract(:reader, "reader/06_read_article_extract_summary")

      article_summary.url = article.url

      all_articles[article_i].ai_summary = article_summary
    end) # , async: true

    db = Local.store_articles_in_db(all_articles, db)
  end)
end) # , async: false # false until we have DB concurrency

fly(:report_news)
